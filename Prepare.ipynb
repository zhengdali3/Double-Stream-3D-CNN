{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the selected frames for each sample are not enough, this preparation process will duplicate frames\n",
    "#in order to meet the depth\n",
    "\n",
    "#In Prepare_TIM, TIM is used to create new frames to meet the depth of 18 frames per sample\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import pandas\n",
    "import Utility as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "image_rows, image_columns, image_depth = 64, 64, 18\n",
    "flow_rows, flow_columns, flow_depth = 144, 120, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSAMM(MM, OF, fps, SAMM_dataset_path):\n",
    "    \n",
    "    SAMM = []\n",
    "    \n",
    "    # data sheet provide info for each sample\n",
    "    dataSheet = pandas.read_excel(io= SAMM_dataset_path + 'SAMM_Micro_FACS_Codes_v2.xlsx', usecols= 'B, D:F, G, J', index_col = 0)\n",
    "    \n",
    "    SAMM_list = []\n",
    "    SAMM_labels = []\n",
    "    SAMM_flow_list = []\n",
    "    directorylisting = os.listdir(SAMM_dataset_path + '/SAMM/')\n",
    "    SAMM_subject_boundary = []\n",
    "    SAMM_subject_boundary.append(0)\n",
    "\n",
    "    for subject in directorylisting:\n",
    "\n",
    "        boundary = SAMM_subject_boundary[-1]\n",
    "        \n",
    "        print(\"SAMM current subjebeloct: \", subject)\n",
    "\n",
    "        for sample in os.listdir(SAMM_dataset_path + '/SAMM/' + subject + '/'):\n",
    "\n",
    "            apex = dataSheet.loc[sample, 'Apex Frame']\n",
    "\n",
    "            emotion = dataSheet.loc[sample, 'Estimated Emotion']\n",
    "\n",
    "            onset = dataSheet.loc[sample, 'Onset Frame']\n",
    "\n",
    "            offset = dataSheet.loc[sample, 'Offset Frame']\n",
    "            \n",
    "            frames = []\n",
    "\n",
    "            frame_count = image_depth\n",
    "\n",
    "            # Image in SAMM will be cropped to height and width shown as below before resizing to 64 x 64\n",
    "            height = 640\n",
    "\n",
    "            width = 960\n",
    "            \n",
    "            # video tensor is used as the data structure for storing selected frames for each sample\n",
    "            video_tensor=np.zeros((frame_count,height,width,3),dtype='float')\n",
    "\n",
    "\n",
    "            if (dataSheet.loc[sample, 'Duration'] >= image_depth and emotion != 'Other'):\n",
    "\n",
    "                label = 0\n",
    "\n",
    "                if(emotion == 'Happiness'):\n",
    "                    label = 1\n",
    "                elif(emotion == 'Surprise'):\n",
    "                    label = 2\n",
    "\n",
    "                count = 0\n",
    "\n",
    "                start = 0\n",
    "\n",
    "                end = 0\n",
    "                \n",
    "                #images around the apex frames are selected\n",
    "\n",
    "                if(apex - onset > (image_depth / 2) and offset - apex >= (image_depth / 2)):\n",
    "\n",
    "                    start = apex - (image_depth / 2)\n",
    "\n",
    "                    end = apex + (image_depth / 2)\n",
    "\n",
    "                elif(apex - onset <= (image_depth / 2)):\n",
    "\n",
    "                    start = onset\n",
    "\n",
    "                    end = onset + image_depth - 1\n",
    "\n",
    "                elif(offset - apex < (image_depth / 2)):\n",
    "\n",
    "                    start = offset - image_depth\n",
    "\n",
    "                    end = offset\n",
    "\n",
    "                for image in os.listdir(SAMM_dataset_path + '/SAMM/'+ subject + '/' + sample + '/'):\n",
    "                    \n",
    "                    imagecode = int(image[-4 - len(str(apex)):-4])\n",
    "                    \n",
    "                    if(imagecode >= start and imagecode < end):\n",
    "                        image_path = SAMM_dataset_path + '/SAMM/' + subject + '/' + sample + '/' + image\n",
    "                        img = cv2.imread(image_path)\n",
    "                        frames.append(img)\n",
    "                        temp = img[:640, : , :]\n",
    "                        video_tensor[count] = temp\n",
    "                        \n",
    "                        count = count + 1\n",
    "                \n",
    "                # if optical flow are required\n",
    "                if(OF == True):\n",
    "                    \n",
    "                    frames_flow = []\n",
    "                    for frame in frames:\n",
    "                        frame_resize = cv2.resize(frame, (flow_columns, flow_rows), interpolation = cv2.INTER_AREA)\n",
    "                        frame_flow = np.float32(frame_resize)\n",
    "                        frames_flow.append(frame_flow)\n",
    "                        \n",
    "                    SAMM_flow = u.opticalFlow(2, frames_flow)\n",
    "                    SAMM_flow = np.asarray(SAMM_flow)\n",
    "                    SAMM_flowarray = np.rollaxis(np.rollaxis(SAMM_flow, 2, 0), 2, 0)\n",
    "                    SAMM_flow_list.append(SAMM_flowarray)\n",
    "                \n",
    "                # if motion magnification are required\n",
    "                if(MM == True):\n",
    "                    frames = u.magnify_video(video_tensor, fps, 0.4, 3, image_rows, image_columns, levels=3, amplification=20)\n",
    "                else:\n",
    "                    resized_frames = []\n",
    "                    for frame in frames:\n",
    "                        imageresize = cv2.resize(frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)\n",
    "                        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                        resized_frames.append(gray_image)\n",
    "                    frames = resized_frames\n",
    "                    \n",
    "                videoarray = np.rollaxis(np.rollaxis(frames, 2, 0), 2, 0)\n",
    "                SAMM_list.append(videoarray)\n",
    "                SAMM_labels.append(label)\n",
    "                boundary = boundary + 1\n",
    "\n",
    "            else:\n",
    "                if(dataSheet.loc[sample, 'Duration'] < image_depth):\n",
    "                    print('Unqualified sample' + sample + ' Duration:' + str(dataSheet.loc[sample, 'Duration']))\n",
    "                else:\n",
    "                    print('Unqualified sample' + sample + ' Emotion: Other')\n",
    "\n",
    "        if(SAMM_subject_boundary[-1] != boundary):\n",
    "            SAMM_subject_boundary.append(boundary)\n",
    "    \n",
    "    # Combine all sample\n",
    "    \n",
    "    SAMMsamples = len(SAMM_list)\n",
    "    SAMM_labels = np_utils.to_categorical(SAMM_labels, 3)\n",
    "    SAMM_data = [SAMM_list,SAMM_labels]\n",
    "    (SAMMframes,SAMM_labels) = (SAMM_data[0], SAMM_data[1])\n",
    "    SAMM_set = np.zeros((SAMMsamples, 1, image_rows, image_columns, image_depth))\n",
    "    SAMM_flow_set = np.zeros((SAMMsamples, 1, flow_rows, flow_columns, flow_depth))\n",
    "\n",
    "    for h in range(SAMMsamples):\n",
    "        SAMM_set[h][0][:][:][:] = SAMMframes[h]\n",
    "        SAMM_flow_set[h][0][:][:][:] = SAMM_flow_list[h]\n",
    "        \n",
    "\n",
    "    SAMM_set = SAMM_set.astype('float32')\n",
    "    SAMM_set -= np.mean(SAMM_set)\n",
    "    SAMM_set /= np.max(SAMM_set)\n",
    "    \n",
    "    SAMM.append(SAMM_set)\n",
    "    SAMM.append(SAMM_labels)\n",
    "    SAMM.append(SAMM_subject_boundary)\n",
    "    if(SAMM_flow_list):\n",
    "        SAMM.append(SAMM_flow_set)\n",
    "        np.save('SAMM/SAMM_flow.npy', SAMM_flow_set)\n",
    "        \n",
    "    np.save('SAMM/SAMM_set.npy', SAMM_set)\n",
    "    np.save('SAMM/SAMM_labels.npy', SAMM_labels)\n",
    "    np.save('SAMM/SAMM_subject_boundary.npy', SAMM_subject_boundary)\n",
    "    \n",
    "    return SAMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareCK(MM, OF, fps, CK_dataset_path):\n",
    "    CK_list = []\n",
    "    CK_labels = []\n",
    "    CK_flow_list = []\n",
    "    CK = []\n",
    "\n",
    "    image_path = CK_dataset_path +  'cohn-kanade-images/'\n",
    "    label_path = CK_dataset_path +  'Emotion/'\n",
    "\n",
    "    CK_dict = {0: 1, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 2}\n",
    "    video_tensor=np.zeros((image_depth,480,640,3),dtype='float')\n",
    "\n",
    "    for subject in os.listdir(label_path):\n",
    "        subject_label_path = os.path.join(label_path, subject)\n",
    "        subject_image_path = os.path.join(image_path, subject)\n",
    "        print(\"CK+ current subject: \", subject)\n",
    "        for test in os.listdir(subject_label_path):\n",
    "            \n",
    "            frames = []\n",
    "            test_label_path = subject_label_path + '/' + test\n",
    "            test_image_path = subject_image_path + '/' + test\n",
    "            if os.listdir(test_label_path):\n",
    "\n",
    "                with open (os.path.join(test_label_path, os.listdir(test_label_path)[-1]), 'r') as rl:\n",
    "                    label = rl.readline()\n",
    "                    label = int(float(label.strip('\\n')))\n",
    "\n",
    "                if label != 0:\n",
    "                    CK_labels.append(CK_dict[label])\n",
    "\n",
    "                    image_dir = os.listdir(test_image_path)\n",
    "\n",
    "                    apex_num = int(len(image_dir) / 2)\n",
    "\n",
    "                    if (apex_num < image_depth):\n",
    "                        \n",
    "                        mul = round(image_depth / apex_num)\n",
    "\n",
    "                        frameNum = 0\n",
    "                        tensorLayer = 0\n",
    "\n",
    "                        while (tensorLayer < image_depth):\n",
    "                            count = 0\n",
    "                            while(count < mul and tensorLayer < image_depth):\n",
    "                                image = cv2.imread(test_image_path + '/' + image_dir[apex_num - frameNum])\n",
    "                                frames.append(image)\n",
    "                                temp = image[:480, :640, :]\n",
    "                                video_tensor[17 - tensorLayer] = temp\n",
    "                                \n",
    "                                \n",
    "                                count = count + 1\n",
    "                                tensorLayer = tensorLayer + 1\n",
    "                            frameNum = frameNum + 1\n",
    "\n",
    "                    else:\n",
    "                        for f in range(image_depth):\n",
    "                            image = cv2.imread(test_image_path + '/' + image_dir[apex_num - f])\n",
    "                            frames.append(image)\n",
    "                            temp = image[:480, :640, :]\n",
    "                            video_tensor[17 - f] = temp\n",
    "                            \n",
    "                    \n",
    "                    if(OF == True):\n",
    "                        frames = reversed(frames)\n",
    "                        frames_flow = []\n",
    "                        for frame in frames:\n",
    "                            frame_resize = cv2.resize(frame, (flow_columns, flow_rows), interpolation = cv2.INTER_AREA)\n",
    "                            frame_flow = np.float32(frame_resize)\n",
    "                            frames_flow.append(frame_flow)\n",
    "                    \n",
    "                        CK_flow = u.opticalFlow(2, frames_flow)\n",
    "\n",
    "                        CK_flow = np.asarray(CK_flow)\n",
    "                        CK_flowarray = np.rollaxis(np.rollaxis(CK_flow, 2, 0), 2, 0)\n",
    "                        CK_flow_list.append(CK_flowarray)\n",
    "                        \n",
    "                    if(MM == True):\n",
    "                        frames = u.magnify_video(video_tensor, fps, 0.4, 3, image_rows, image_columns, levels=3, amplification=20)\n",
    "                    else:\n",
    "                        resized_frames = []\n",
    "                        for frame in frames:\n",
    "                            imageresize = cv2.resize(frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)\n",
    "                            grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                            resized_frames.append(gray_image)\n",
    "                        frames = resized_frames\n",
    "                        \n",
    "                    videoarray = np.rollaxis(np.rollaxis(frames, 2, 0), 2, 0)\n",
    "                    CK_list.append(videoarray)\n",
    "                    \n",
    "    CK_labels = np.asarray(CK_labels)\n",
    "    CK_labels = np_utils.to_categorical(CK_labels, 3)\n",
    "\n",
    "    CKsamples = len(CK_list)\n",
    "    CK_data = [CK_list, CK_labels]\n",
    "    (CKframes, CK_labels) = (CK_data[0], CK_data[1])\n",
    "    CK_set = np.zeros((CKsamples, 1, image_rows, image_columns, image_depth))\n",
    "    CK_flow_set = np.zeros((CKsamples, 1, flow_rows, flow_columns, flow_depth))\n",
    "\n",
    "    for h in range(CKsamples):\n",
    "        CK_set[h][0][:][:][:] = CKframes[h]\n",
    "        CK_flow_set[h][0][:][:][:] = CK_flow_list[h]\n",
    "\n",
    "    CK_set = CK_set.astype('float32')\n",
    "    CK_set -= np.mean(CK_set)\n",
    "    CK_set /= np.max(CK_set)\n",
    "\n",
    "#     CK_flow_set = CK_flow_set.astype('float32')\n",
    "#     CK_flow_set -= np.mean(CK_flow_set)\n",
    "#     CK_flow_set /= np.max(CK_flow_set)\n",
    "\n",
    "    CK.append(CK_set)\n",
    "    CK.append(CK_labels)\n",
    "    if(CK_flow_list):\n",
    "        CK.append(CK_flow_set)\n",
    "\n",
    "        np.save('CK+/CK_flow.npy', CK_flow_set)\n",
    "        \n",
    "    np.save('CK+/CK_set.npy', CK_set)\n",
    "    np.save('CK+/CK_labels.npy', CK_labels)\n",
    "        \n",
    "    return CK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSMIC(MM, OF, fps, SMIC_dataset_path):\n",
    "    SMIC = []\n",
    "    SMIC_list = []\n",
    "    SMIC_labels = []\n",
    "    SMIC_flow_list = []\n",
    "    directorylisting = os.listdir(SMIC_dataset_path)\n",
    "    subject_boundary = []\n",
    "    \n",
    "    video_tensor = np.zeros((image_depth, 144, 120, 3), dtype='float')\n",
    "\n",
    "    subject_boundary.append(0)\n",
    "\n",
    "    for subject_path in directorylisting:\n",
    "        \n",
    "        print(\"SMIC current subject: \", subject_path)\n",
    "\n",
    "        boundary = subject_boundary[-1]\n",
    "        path = []\n",
    "\n",
    "        path.append(SMIC_dataset_path + subject_path + '/micro/negative/') \n",
    "        path.append(SMIC_dataset_path + subject_path + '/micro/positive/')\n",
    "        path.append(SMIC_dataset_path + subject_path + '/micro/surprise/')\n",
    "\n",
    "        # for each label in one subject\n",
    "\n",
    "        for label in range(3):\n",
    "\n",
    "            for example in os.listdir(path[label]):\n",
    "\n",
    "                example_path = path[label] + example\n",
    "\n",
    "                frames = []\n",
    "                framelisting = os.listdir(example_path)\n",
    "                framerange = image_depth\n",
    "                i = 0\n",
    "\n",
    "                if(len(framelisting) < framerange):\n",
    "                    framerange = len(framelisting)\n",
    "\n",
    "                for frame in range(framerange):\n",
    "                    imagepath = example_path + \"/\" + framelisting[frame]\n",
    "                    image = cv2.imread(imagepath)\n",
    "                    temp = image[:144, :120, :]\n",
    "                    video_tensor[i] = temp\n",
    "                    i = i + 1\n",
    "                    frames.append(image)\n",
    "\n",
    "                for duplicate in range(image_depth - framerange):\n",
    "                    imagepath = example_path + \"/\" + framelisting[framerange - 1]\n",
    "                    image = cv2.imread(imagepath)\n",
    "                    temp = image[:144, :120, :]\n",
    "                    video_tensor[i] = temp\n",
    "                    i = i + 1\n",
    "                    frames.append(image)\n",
    "                \n",
    "                if(OF == True):\n",
    "\n",
    "                    frames_flow = []\n",
    "\n",
    "                    for frame in frames:\n",
    "                        frame_resize = cv2.resize(frame, (flow_columns, flow_rows), interpolation = cv2.INTER_AREA)\n",
    "                        frame_flow = np.float32(frame_resize)\n",
    "                        frames_flow.append(frame_flow)\n",
    "                    \n",
    "                    SMIC_flow = u.opticalFlow(2, frames_flow)\n",
    "\n",
    "                    SMIC_flow = np.asarray(SMIC_flow)\n",
    "                    SMIC_flowarray = np.rollaxis(np.rollaxis(SMIC_flow, 2, 0), 2, 0)\n",
    "                    SMIC_flow_list.append(SMIC_flowarray)\n",
    "                \n",
    "                if(MM == True):\n",
    "                    frames = u.magnify_video(video_tensor, fps, 0.4, 3, image_rows, image_columns, levels=3, amplification=20)\n",
    "                else:\n",
    "                    resized_frames = []\n",
    "                    \n",
    "                    for frame in frames:\n",
    "                        imageresize = cv2.resize(frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)\n",
    "                        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)\n",
    "                        resized_frames.append(gray_image)\n",
    "                    frames = resized_frames\n",
    "                    \n",
    "\n",
    "                frames = np.asarray(frames)\n",
    "                videoarray = np.rollaxis(np.rollaxis(frames, 2, 0), 2, 0)\n",
    "                SMIC_list.append(videoarray)\n",
    "                SMIC_labels.append(label)\n",
    "                boundary = boundary + 1\n",
    "\n",
    "        subject_boundary.append(boundary)\n",
    "    \n",
    "    SMIC_labels = np.asarray(SMIC_labels)\n",
    "    SMIC_labels = np_utils.to_categorical(SMIC_labels, 3)\n",
    "\n",
    "    SMICsamples = len(SMIC_list)\n",
    "    SMIC_data = [SMIC_list, SMIC_labels]\n",
    "    (SMICframes, SMIC_labels) = (SMIC_data[0], SMIC_data[1])\n",
    "    SMIC_set = np.zeros((SMICsamples, 1, image_rows, image_columns, image_depth))\n",
    "    SMIC_flow_set = np.zeros((SMICsamples, 1, flow_rows, flow_columns, flow_depth))\n",
    "\n",
    "    for h in range(SMICsamples):\n",
    "        SMIC_set[h][0][:][:][:] = SMICframes[h]\n",
    "        SMIC_flow_set[h][0][:][:][:] = SMIC_flow_list[h]\n",
    "\n",
    "    SMIC_set = SMIC_set.astype('float32')\n",
    "    SMIC_set -= np.mean(SMIC_set)\n",
    "    SMIC_set /= np.max(SMIC_set)\n",
    "    \n",
    "    SMIC.append(SMIC_set)\n",
    "    SMIC.append(SMIC_labels)\n",
    "    SMIC.append(subject_boundary)\n",
    "    if(SMIC_flow_list):\n",
    "        SMIC.append(SMIC_flow_set)\n",
    "        np.save('SMIC/SMIC_flow.npy', SMIC_flow_set)\n",
    "        \n",
    "    np.save('SMIC/SMIC_set.npy', SMIC_set)\n",
    "    np.save('SMIC/SMIC_labels.npy', SMIC_labels)\n",
    "    np.save('SMIC/SMIC_subject_boundary.npy', subject_boundary)\n",
    "        \n",
    "\n",
    "    return SMIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
